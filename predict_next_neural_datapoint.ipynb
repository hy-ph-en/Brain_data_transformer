{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "application/javascript": "IPython.notebook.set_autosave_interval(180000)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 180 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mnzp0\\AppData\\Local\\Temp\\ipykernel_17332\\4197309928.py:12: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython.display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "#matplotlib.use('Agg')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib tk\n",
    "%autosave 180\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "############### PREDICT NEXT NEURAL TIME SERIES POINT ################ \n",
    "\n",
    "Here we are just trying to get the transformer to learn the dynamics of raw (i.e. PCA denoised) neural time series.\n",
    "\n",
    "#### Step 1: One brain area\n",
    "\n",
    "- input shape [time_points] = [40000]    # this is just a single time series from the visualizion notebook\n",
    "- label: [time_points[1:]]                # here we predict the time series but shifted by 1\n",
    "\n",
    "This is exactly what transformers are developed to do, so we shouldn't have to do too much work to adapt them. We can also smooth or bin the neural data as it's abit noisy. \n",
    "\n",
    "##### Major challenges:\n",
    "\n",
    "1. Figure out how to feed continous time series into the transformer.\n",
    "\n",
    "There are some methods already out there\n",
    "\n",
    "https://huggingface.co/blog/time-series-transformers\n",
    "\n",
    "https://huggingface.co/docs/transformers/model_doc/time_series_transformer\n",
    "\n",
    "\n",
    "#### Step 2: Multiple brain areas\n",
    "\n",
    "- input shape [time_points, n_areas] = [40000, 30]     #  \n",
    "- label: [time_points[1:], 30]                         # \n",
    "\n",
    "##### Major challenges:\n",
    "\n",
    "1. So we would need to extend the above to work with multiple cortical areas...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import math\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import glob\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading The Data\n",
    "root_dir = 'data'\n",
    "mouse_id = 'IA1'\n",
    "session_id = 'Feb_16'\n",
    "\n",
    "# load the raw data from each neural area\n",
    "animal_dir = os.path.join(root_dir, mouse_id, session_id)\n",
    "\n",
    "# find the file using glob that has \"wholestack.npz\" in it\n",
    "fname = glob.glob(os.path.join(animal_dir, '*wholestack.npz'))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How much are we working with - Reducing initial computational time\n",
    "data_slice = 40000\n",
    "\n",
    "# Load data and verify shape\n",
    "data_stack = np.load(fname, allow_pickle=True)\n",
    "data = data_stack['whole_stack'].T  # Shape: (40000, 16)\n",
    "neural_data = torch.tensor(data[:data_slice], dtype=torch.float32)  # Slice to (1000, 16) - Cut down on the computational complexity\n",
    "\n",
    "\n",
    "# After loading data and slicing to (1000, 16)\n",
    "# Compute mean/std on training data only\n",
    "training_slice = int(data_slice*0.7)\n",
    "train_data = neural_data[:training_slice]  # 70% of 1000 = 700 samples\n",
    "mean = train_data.mean(dim=0)\n",
    "std = train_data.std(dim=0)\n",
    "neural_data_normalized = (neural_data - mean) / (std + 1e-8)\n",
    "\n",
    "window_size = 10  # Adjust with some testing\n",
    "\n",
    "# Regenerate inputs/targets with normalized data\n",
    "inputs, targets = [], []\n",
    "for i in range(window_size, len(neural_data_normalized)):\n",
    "    inputs.append(neural_data_normalized[i-window_size:i])\n",
    "    targets.append(neural_data_normalized[i])\n",
    "\n",
    "\n",
    "# Convert to tensors\n",
    "inputs = torch.stack(inputs)    # Shape: (N, window_size, 16)\n",
    "targets = torch.stack(targets)  # Shape: (N, 16)\n",
    "\n",
    "# Split into train/val/test (70/15/15)\n",
    "train_size = int(0.7 * len(inputs))\n",
    "val_size = int(0.15 * len(inputs))\n",
    "test_size = len(inputs) - train_size - val_size\n",
    "\n",
    "train_dataset = TensorDataset(inputs[:train_size], targets[:train_size])\n",
    "val_dataset = TensorDataset(inputs[train_size:train_size+val_size], targets[train_size:train_size+val_size])\n",
    "test_dataset = TensorDataset(inputs[-test_size:], targets[-test_size:])\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class NeuralTransformer(nn.Module):\n",
    "    def __init__(self, input_dim=16, d_model=64, nhead=4, num_layers=3):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        self.norm = nn.LayerNorm(d_model)  # Add LayerNorm\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model, nhead, batch_first=True, dropout=0.1\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.decoder = nn.Linear(d_model, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) * math.sqrt(self.d_model)\n",
    "        x = self.norm(x)  # Apply LayerNorm\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=1)  # Use mean instead of last timestep\n",
    "        return self.decoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.0349\n",
      "Epoch 2, Loss: 0.0156\n",
      "Epoch 3, Loss: 0.0121\n",
      "Epoch 4, Loss: 0.0107\n",
      "Epoch 5, Loss: 0.0103\n",
      "Epoch 6, Loss: 0.0081\n",
      "Epoch 7, Loss: 0.0116\n",
      "Epoch 8, Loss: 0.0078\n",
      "Epoch 9, Loss: 0.0092\n",
      "Epoch 10, Loss: 0.0078\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = NeuralTransformer().to(device)\n",
    "criterion = nn.HuberLoss()  # Replace MSE with Huber loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)  # Lower learning rate\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_inputs, batch_targets in train_loader:\n",
    "        batch_inputs, batch_targets = batch_inputs.to(device), batch_targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_inputs)\n",
    "        loss = criterion(outputs, batch_targets)\n",
    "        \n",
    "        if torch.isnan(loss):\n",
    "            print(\"NaN detected. Stopping training.\")\n",
    "            break\n",
    "            \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
