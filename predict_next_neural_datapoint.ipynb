{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "IPython.notebook.set_autosave_interval(180000)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 180 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mnzp0\\AppData\\Local\\Temp\\ipykernel_17332\\4197309928.py:12: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython.display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "#matplotlib.use('Agg')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib tk\n",
    "%autosave 180\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "############### PREDICT NEXT NEURAL TIME SERIES POINT ################ \n",
    "\n",
    "Here we are just trying to get the transformer to learn the dynamics of raw (i.e. PCA denoised) neural time series.\n",
    "\n",
    "#### Step 1: One brain area\n",
    "\n",
    "- input shape [time_points] = [40000]    # this is just a single time series from the visualizion notebook\n",
    "- label: [time_points[1:]]                # here we predict the time series but shifted by 1\n",
    "\n",
    "This is exactly what transformers are developed to do, so we shouldn't have to do too much work to adapt them. We can also smooth or bin the neural data as it's abit noisy. \n",
    "\n",
    "##### Major challenges:\n",
    "\n",
    "1. Figure out how to feed continous time series into the transformer.\n",
    "\n",
    "There are some methods already out there\n",
    "\n",
    "https://huggingface.co/blog/time-series-transformers\n",
    "\n",
    "https://huggingface.co/docs/transformers/model_doc/time_series_transformer\n",
    "\n",
    "\n",
    "#### Step 2: Multiple brain areas\n",
    "\n",
    "- input shape [time_points, n_areas] = [40000, 30]     #  \n",
    "- label: [time_points[1:], 30]                         # \n",
    "\n",
    "##### Major challenges:\n",
    "\n",
    "1. So we would need to extend the above to work with multiple cortical areas...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import glob\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading The Data\n",
    "root_dir = 'data'\n",
    "mouse_id = 'IA1'\n",
    "session_id = 'Feb_16'\n",
    "\n",
    "# load the raw data from each neural area\n",
    "animal_dir = os.path.join(root_dir, mouse_id, session_id)\n",
    "\n",
    "# find the file using glob that has \"wholestack.npz\" in it\n",
    "fname = glob.glob(os.path.join(animal_dir, '*wholestack.npz'))[0]\n",
    "\n",
    "data_stack = np.load(fname, allow_pickle=True)\n",
    "data = data_stack['whole_stack']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice - What amount to look at to reduce the computational complexity for the 40,000 - Increase Later\n",
    "slice = 1000\n",
    "\n",
    "if slice == 0:  # Default to the length of the data if a slice is not being used\n",
    "    slice = len(torch.tensor(data[1]))\n",
    "\n",
    "\n",
    "# Starting off lets just use the start of the tensor\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "neural_data = torch.tensor(data, dtype=torch.float32)[:slice]\n",
    "seq_len = slice\n",
    "\n",
    "time_steps = torch.arange(seq_len, dtype=int)  # Creates a tensor [0, 1, 2, ..., seq_len-1]\n",
    "\n",
    "# Doing this incorrect, need to seperate the different areas not just the two area gaps\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = TensorDataset(neural_data)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to create Test, Validation, and Train Sets\n",
    "def preprocess_data(data, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, random_state=None):\n",
    "    \"\"\"\n",
    "    Preprocess neural data by splitting into train/val/test sets and normalizing.\n",
    "    \n",
    "    Args:\n",
    "        data: Input array of shape [spatial_dims, time_points]\n",
    "        train_ratio: Proportion of data for training (default: 0.7)\n",
    "        val_ratio: Proportion of data for validation (default: 0.15)\n",
    "        test_ratio: Proportion of data for testing (default: 0.15)\n",
    "        random_state: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        dict: Contains train, val, and test splits (both normalized and raw)\n",
    "    \"\"\"\n",
    "    # Verify ratios sum to 1\n",
    "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-10, \"Ratios must sum to 1\"\n",
    "    \n",
    "    # Get dimensions\n",
    "    n_spatial, n_time = data.shape\n",
    "    \n",
    "    # Create indices and optionally shuffle\n",
    "    indices = np.arange(n_time)\n",
    "    if random_state is not None:\n",
    "        rng = np.random.RandomState(random_state)\n",
    "        rng.shuffle(indices)\n",
    "    \n",
    "    # Calculate split points\n",
    "    train_end = int(n_time * train_ratio)\n",
    "    val_end = int(n_time * (train_ratio + val_ratio))\n",
    "    \n",
    "    # Split the data\n",
    "    train_data = data[:, indices[:train_end]]\n",
    "    val_data = data[:, indices[train_end:val_end]]\n",
    "    test_data = data[:, indices[val_end:]]\n",
    "    \n",
    "    # Initialize and fit scaler on training data\n",
    "    scaler = StandardScaler()\n",
    "    train_2d = train_data.transpose()\n",
    "    scaler.fit(train_2d)\n",
    "    \n",
    "    # Transform all splits\n",
    "    train_norm = scaler.transform(train_2d).transpose()\n",
    "    val_norm = scaler.transform(val_data.transpose()).transpose()\n",
    "    test_norm = scaler.transform(test_data.transpose()).transpose()\n",
    "    \n",
    "    return {\n",
    "        'train': train_norm,\n",
    "        'val': val_norm,\n",
    "        'test': test_norm,\n",
    "        'train_raw': train_data,\n",
    "        'val_raw': val_data,\n",
    "        'test_raw': test_data,\n",
    "        'scaler': scaler\n",
    "    }\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
